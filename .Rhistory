first_fit <- lm(mpg ~ ., data = mtcars)
simple_fit <- lm(mpg ~ wt+cyl+disp+hp., data = mtcars)
simple_fit <- lm(mpg ~ wt+cyl+disp+hp, data = mtcars)
anova(simple_fit, best_fit, first_fit)
simple_fit <- lm(mpg ~ am, data = mtcars)
anova(simple_fit, best_fit, first_fit)
par(mrow = c(2,2))
par(mfrow = c(2,2))
plot(best_fit)
t.test(autoCars$mpg, manualCars$mpg)
View(manualCars)
View(autoCars)
View(mtcars)
library(swirl)
swirl()
fit$residual
fit$residuals
fit <- lm(child~parent,galton)
fit$residuals
summary(fit)
mean(fit$residuals)
cov(fit$residuals, galton$parent)
ols.ic <- fit$coef[1]
ols.slope <-fit$coef[2]
lhs - rhs
all.equal(lhs,rhs)
varChild <- var(galton$child)
varRes <- var(fit$residuals)
varEst <- var(est(ols.slope,ols.ic))
all.equal(varChild, varRes+varEst)
efit <- lm(accel ~ mag+dist, attenu)
mean(efit$residuals)
cov(efit$residuals, attenu$mag)
cov(efit$residuals, attenu$dist)
cor(gpa_nor,gch_nor)
l_nor <- lm(gch_nor ~ gpa_nor)
fit <- lm(child~parent, galton)
sqrt(sum(fit$residuals^2) / (n-2))
summary(fit$sigma)
summary(fit$sigma)
summary(fit)$sigma
summary(fit)$sigma
sqrt(deviance(fit)/(n-2))
mu <- mean(galton$child)
sTot <- sum((galton$child - mu)^2)
sRes <- deviance(fit)
1 - sRes/sTot
summary(fit)$r.squared
cor(galton$parent, galton$child)^2
ones <- rep(1, nrow(galton))
lm(child ~ ones + parent-1, galton)
lm(child ~ parent, galton)
lm(child~1,galton)
head(trees)
fit <- lm(Volume ~ Girth + Height + Constant - 1, trees)
trees2 <- eliminate("girth", trees)
trees2 <- eliminate("Girth", trees)
head(trees2)
fit2 <- lm(Volume ~ Height + Constant -1, trees2)
lapply(list(fit, fit2), coef)
install.packages("rattle")
install.packages('rpart.plot')
install.packages('ElemStatLearn')
install.packages('pgmm')
install.packages('caret')
install.packages('randomForest')
install.packages('e1071')
install.packages('gbm')
install.packages('survival')
install.packages('splines')
install.packages('plyr')
install.packages('caTools')
install.packages("splines")
library(kernlab)
install.packages("kernlab")
library(kernlab)
data(spam)
setseed(333)
set seed(333)
set.seed(333)
smallSpam <- spam[sample(dim(spam)[1],size=10),]
spamLabel <- (smallSpam$type=="spam")*1 +1
plot(smallSpam$capitalAve,col=spamLabel)
spamLabel <- (smallSpam$type=="spam")
plot(smallSpam$capitalAve,col=spamLabel)
spamLabel <- (smallSpam$type=="spam") +1
plot(smallSpam$capitalAve,col=spamLabel)
prediction <- rep(NA,length(x))
prediction
rm(list=ls())
library(caret)
library(caret)
library(ISLR)
library(splines)
library(caret)
library(randomForest)
library(doParallel)
install.packages("doParallel")
library(doParallel)
set.seed(6789)
train.dataset <- read.csv("~/Machine_Learning/pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
train.dataset <- read.csv("/Machine_Learning/pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
train.dataset <- read.csv("~/MOOC/Machine_Learning/pml-training.csv")
View(train.dataset)
summary(train.dataset)
train.dataset <- read.csv("~/MOOC/Machine_Learning/pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
View(train.dataset)
rm(train.dataset)
train.dataset <- read.csv("~/MOOC/Machine_Learning/pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
View(train.dataset)
test.dataset <- read.csv('~/MOOC/Machine_Learning/pml-testing.csv', na.strings=c("NA","#DIV/0!", ""))
# Check size of training and test dataset
dim(train.dataset)
dim(test.dataset)
View(test.dataset)
View(train.dataset)
View(train.dataset)
train1.dataset<-train.dataset[,colSums(is.na(train.dataset)) == 0]
test1.dataset <-test.dataset[,colSums(is.na(test.dataset)) == 0]
View(train1.dataset)
train.dataset<-train.dataset[,colSums(is.na(train.dataset)) == 0]
test.dataset <-test.dataset[,colSums(is.na(test.dataset)) == 0]
train.dataset <-train.dataset[,c(8:60)]
test.dataset <-test.dataset[,c(8:60)]
cor_train.matrix <- cor(na.omit(train.dataset[sapply(train.dataset, is.numeric)]))
View(cor_train.matrix)
cor_train_matrix <- cor(na.omit(train.dataset[sapply(train.dataset, is.numeric)]))
rm_cor_train <- findCorrelation(cor_train_matrix, cutoff = .80, verbose = TRUE)
library(caret)
library(randomForest)
library(doParallel)
rm_cor_train <- findCorrelation(cor_train_matrix, cutoff = .80, verbose = TRUE)
View(train.dataset)
train1.dataset <- train.dataset[,-rm_cor_train]
View(train1.dataset)
dim(train1.dataset)
unique(train1.dataset$classe)
rm_cor_train <- findCorrelation(cor_train_matrix, cutoff = .80, verbose = TRUE)
training_dataset <- train_dataset[,-rm_cor_train]
dim(training_dataset)
unique(training_dataset$classe)
training_dataset <- train.dataset[,-rm_cor_train]
dim(training_dataset)
unique(training_dataset$classe)
part.tr <- createDataPartition(y = train_dataset$classe, p=0.7,list=FALSE)
part.training <- train.dataset[part.tr,]
part.testing <- train.dataset[-part.tr,]
part.tr <- createDataPartition(y = training_dataset$classe, p=0.7,list=FALSE)
part.training <- training_dataset[part.tr,]
part.testing <- training_dataset[-part.tr,]
dim(part.training)
dim(part.testing)
View(part.training)
View(part.testing)
plot(training$classe, col=rainbow(5), main="Levels of the variable classe in the training dataset",
xlab="classe levels", ylab= "Frequency")
plot(part.training$classe, col=rainbow(5), main="Levels of the variable classe in the training dataset",
xlab="classe levels", ylab= "Frequency")
plot(part.training$classe, col=rainbow(5), main="Classe Frequency Levels in the training dataset",
xlab="classe levels", ylab= "Frequency")
plot(part.training$classe, col=rainbow(6), main="Classe Frequency Levels in the training dataset",
xlab="classe levels", ylab= "Frequency")
plot(part.training$classe, col=rainbow(8), main="Classe Frequency Levels in the training dataset",
xlab="classe levels", ylab= "Frequency")
plot(part.training$classe, col=rainbow(7), main="Classe Frequency Levels in the training dataset",
xlab="classe levels", ylab= "Frequency")
plot(part.training$classe, col=rainbow(7), main="Classe Frequency Levels in the training dataset",
xlab="classe levels", ylab= "Frequency")
plot(part.training$classe, col=rainbow(9), main="Classe Frequency Levels in the training dataset",
xlab="classe levels", ylab= "Frequency")
plot(part.training$classe, col=rainbow(3), main="Classe Frequency Levels in the training dataset",
xlab="classe levels", ylab= "Frequency")
fit.model.tree <- train(classe ~ .,method="rpart",data=part.training)
fit.model.rf <- train(classe ~ .,  method = 'rf',data=part.training)
library(randomForest)
rfNews()
library(caret)
library(randomForest)
library(caret)
fit.model.tree <- train(classe ~ .,method="rpart",data=part.training)
fit.model.lda <-train(classe ~ ., method = 'lda',data = part.training)
fit.model.tree <- train(classe ~ .,method="rpart",data=part.training)
View(test1.dataset)
fit.model.rf <- train(randomForest(Classe ~ ., data=part.training, importance=TRUE, proximity=TRUE))
fit.model.rf <- train(classe ~ .,  method = 'rf',data=part.training)
library(caret)
library(randomForest)
library(rpart)
library(MASS)
library(doParallel)
fit.model.rf <- train(classe ~ .,  method = 'rf',data=part.training)
fit.model.rf <- train(classe ~ .,  method = 'rf',data=part.training)
library(caret)
library(randomForest)
library(rpart)
library(UsingR)
library(doParallel)
install.packages("UsingR")
install.packages("Slidify")
install.packages("devtools")
install_github('slidify', 'ramnathv')
library(caret)
library(randomForest)
library(rpart)
library(UsingR)
library(doParallel)
library(devtools)
install_github('slidify', 'ramnathv')
install_github('slidifyLibraries', 'ramnathv')
install.packages("manipulate")
install.packages("shiny")
install.packages("rChart")
require(rCharts)
install_github('ramnathv/rCharts')
install.packages("yhat")
install.packages("googleVis")
install.packages("googlesheets")
install_github("ropensci/plotly")
install.packages("viridis")
install.packages("viridis")
install_github("sjmgarnier/viridis")
install_github("ropensci/plotly")
install.packages("curl")
install_github("sjmgarnier/viridis")
library(devtools)
install_github("sjmgarnier/viridis")
install.packages(c("bit64", "boot", "class", "cluster", "codetools", "CORElearn", "crayon", "devtools", "downloader", "dplyr", "foreign", "git2r", "gridExtra", "KernSmooth", "lattice", "lme4", "maps", "MASS", "Matrix", "mgcv", "nlme", "nnet", "psych", "quantmod", "quantreg", "raster", "Rcpp", "Rfacebook", "rJava", "RMySQL", "rpart", "rplos", "rversions", "signal", "SparseM", "spatial", "stargazer", "tidyr", "TTR", "twitteR", "XML", "xml2"))
install.packages(c("bit64", "boot", "class", "cluster", "codetools",
)
)
library(caret)
library(randomForest)
library(rpart)
library(UsingR)
library(doParallel)
library(devtools)
rm(list=ls())
set.seed(6789)
# Data in training set containing Null and "DIV/0!" values will be replaced with "NA".
# Load the training dataset and replace Null and "DIV/0!" with "NA"
train.dataset <- read.csv("~/MOOC/Machine_Learning/pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
test.dataset <- read.csv('~/MOOC/Machine_Learning/pml-testing.csv', na.strings=c("NA","#DIV/0!", ""))
dim(train.dataset)
dim(test.dataset)
```{r, echo=TRUE}
# Remove colums with all NA values
train.dataset<-train.dataset[,colSums(is.na(train.dataset)) == 0]
test.dataset <-test.dataset[,colSums(is.na(test.dataset)) == 0]
```
train.dataset<-train.dataset[,colSums(is.na(train.dataset)) == 0]
test.dataset <-test.dataset[,colSums(is.na(test.dataset)) == 0]
train.dataset <-train.dataset[,c(8:60)]
test.dataset <-test.dataset[,c(8:60)]
cor_train_matrix <- cor(na.omit(train.dataset[sapply(train.dataset, is.numeric)]))
rm_cor_train <- findCorrelation(cor_train_matrix, cutoff = .80, verbose = TRUE)
training_dataset <- train.dataset[,-rm_cor_train]
dim(training_dataset)
unique(training_dataset$classe)
part.tr <- createDataPartition(y = training_dataset$classe, p=0.7,list=FALSE)
part.training <- training_dataset[part.tr,]
part.testing <- training_dataset[-part.tr,]
dim(part.training)
dim(part.testing)
fit.model.tree <- train(classe ~ .,method="rpart",data=part.training)
fit.model.lda <-train(classe ~ ., method = 'lda',data = part.training)
fit.model.rf <- train(randomForest(Classe ~ ., data=part.training, importance=TRUE, proximity=TRUE))
fit.model.rf <- train(classe ~ .,  method = 'rf',data=part.training)
View(training_dataset)
View(part.testing)
View(train.dataset)
View(cor_train_matrix)
print(fit.model.rf)
k.fit.model.rf <- train(classe ~ .,  method = 'rf',data=training,
trControl = trainControl(method="cv",number=5, allowParallel = TRUE) )
library(caret)
library(randomForest)
library(rpart)
library(UsingR)
library(doParallel)
library(devtools)
k.fit.model.rf <- train(classe ~ .,  method = 'rf',data=training,
trControl = trainControl(method="cv",number=5, allowParallel = TRUE) )
k.fit.model.rf <- train(classe ~ .,  method = 'rf',data=part.training,
trControl = trainControl(method="cv",number=5, allowParallel = TRUE) )
print(k.fit.model.rf)
plot(k.fit.model.rf, main="Resampling Results - Cross Validation")
prediction_randomforest <- predict(k.fit.model.rf, testing)
prediction_randomforest <- predict(k.fit.model.rf, part.testing)
table(prediction_randomforest, part.testing$classe)
postResample(prediction_rf, part.testing$classe)
postResample(prediction_randomforest, part.testing$classe)
View(test.dataset)
prediction_result <- predict(k.fit.model.rf, test.dataset)
print(prediction_result)
table(prediction_randomforest, part.testing$classe)
confusionMatrix(part.testing$classe, prediction_randomforest)
print(fit.model.tree)
print(fit.model.lda)
prediction_result <- predict(k.fit.model.rf, test.dataset);
print(prediction_result);
pml_write_files(as.vector(prediction.result))
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
pml_write_files(as.vector(prediction.result))
pml_write_files(as.vector(prediction_result))
pml_write_files(as.vector(prediction_result))
setwd("~/MOOC/Machine_Learning")
pml_write_files(as.vector(prediction_result))
pml_write_files(as.vector(prediction_result))
plot(part.training$classe, col=3, main="Classe Frequency Levels (training dataset)",
xlab="Classe Levels", ylab= "Frequency")
plot(part.training$classe, col=4, main="Classe Frequency Levels (training dataset)",
xlab="Classe Levels", ylab= "Frequency")
plot(part.training$classe, col=5, main="Classe Frequency Levels (training dataset)",
xlab="Classe Levels", ylab= "Frequency")
install.packages("AppliedPredictiveModeling")
install.packages("ElemStatLearn")
install.packages("pgmm")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
data <- segmentationOriginal
set.seed(125)
inTrain <- data$Case == "Train"
View(segmentationOriginal)
summary(segmentationOriginal)
str(segmentationOriginal)
trainData <- data[inTrain, ]
testData <- data[!inTrain, ]
View(testData)
View(trainData)
cartModel <- train(Class ~ ., data=trainData, method="rpart")
cartModel$finalModel
library(pgmm)
data(olive)
olive = olive[,-1]
View(olive)
data(olive)
data(olive)
olive = olive[,-1]
data(olive)
View(olive)
olive = olive[,-1]
View(olive)
newdata = as.data.frame(t(colMeans(olive)))
View(newdata)
predict(treeModel, newdata) # 2.875
treeModel <- train(Area ~ ., data=olive, method="rpart2")
treeModel
rm(list=ls())
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
str train
str(train)
summary train
summary(train)
View(SAheart)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed=13234
install.packages("DDPQuiz3")
logitModel <- train(chd ~ age + alcohol + obesity + tobacco +
typea + ldl , data = trainSA, method="glm",
family="binomial")
library(caret)
logitModel <- train(chd ~ age + alcohol + obesity + tobacco +
typea + ldl , data = trainSA, method="glm",
family="binomial")
logitModel
missClass <- functlogitModelion(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
predictTrain <- predict(logitModel, trainSA)
predictTest <- predict(logitModel, testSA)
missClass(trainSA$chd, predictTrain)
missClass <- functlogitModelion(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass <- functlogitModelion(values,prediction) {sum(((prediction > 0.5)*1) != values)/length(values)}
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.train)
head(vowel.test)
dim(vowel.train) # 528  11
dim(vowel.test) # 462  11
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
library(randomForest)
modelRf <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
varImp(modelRf)
order(varImp(modelRf), decreasing=T)
rm(list=ls())
require(data.table)
# library(sqldf)
library(dplyr)
library(DT)
library(rCharts)
data <- fread("~/MOOC/Developing_Data_Products/data/International_LPI_from_2007_to_2014-2.xlsx")
head(data)
require(xlsx)
data <- read.xlsx("~/MOOC/Developing_Data_Products/data/International_LPI_from_2007_to_2014-2.xlsx", sheetName = "Sheet1")
getwd()
data <- read.xlsx("~/MOOC/Developing_Data_Products/data/International_LPI_from_2007_to_2014-2.xlsx", sheetName = "2014")
data2 <- read.xlsx2("~/MOOC/Developing_Data_Products/data/International_LPI_from_2007_to_2014-2.xlsx", sheetName = "2014")
View(data)
View(data2)
data <- read.xlsx("~/MOOC/Developing_Data_Products/data/International_LPI_from_2007_to_2014-2.xlsx", sheetName = "2014", startRow = 3)
View(data)
data <- read.xlsx("~/MOOC/Developing_Data_Products/data/International_LPI_from_2007_to_2014-2.xlsx", sheetName = "2014")
View(data)
data <- read.xlsx("~/MOOC/Developing_Data_Products/data/International_LPI_from_2007_to_2014-2.xlsx", sheetName = "2014", skip = 2)
View(data)
View(data)
data <- read.xlsx("~/MOOC/Developing_Data_Products/data/International_LPI_from_2007_to_2014-2.xlsx", sheetName = "2014")
View(data)
View(data)
data <- read.xlsx("~/MOOC/Developing_Data_Products/data/International_LPI_from_2007_to_2014-2.xlsx", sheetName = "2014")
View(data)
View(data)
data_tmp <- read.xlsx("~/MOOC/Developing_Data_Products/data/International_LPI_from_2007_to_2014-2.xlsx", sheetName = "2014")
colnames(data_tmp)[1] <- 'country'
colnames(data_tmp)[2] <- 'coountry_code'
View(data_tmp)
colnames(data_tmp)[1] <- 'country'
colnames(data_tmp)[2] <- 'coountry.code'
colnames(data_tmp)[3] <- 'overall.LPI.score'
colnames(data_tmp)[4] <- 'overall.LPI.score.lowerBound'
colnames(data_tmp)[5] <- 'overall.LPI.score.upperBound'
colnames(data_tmp)[6] <- 'overall.LPI.rank'
colnames(data_tmp)[7] <- 'overall.LPI.rank.lowerBound'
colnames(data_tmp)[8] <- 'overall.LPI.rank.upperBound'
colnames(data_tmp)[9] <- 'overall.LPI.rank.percentHighestPerformer'
colnames(data_tmp)[10] <- 'customs.score'
colnames(data_tmp)[11] <- 'customs.rank'
colnames(data_tmp)[12] <- 'infrastructure.score'
colnames(data_tmp)[13] <- 'infrastructure.rank'
colnames(data_tmp)[14] <- 'international.shipment.score'
colnames(data_tmp)[15] <- 'international.shipment.rank'
colnames(data_tmp)[16] <- 'logisticsQualityCompetence.score'
colnames(data_tmp)[17] <- 'logisticsQualityCompetence.rank'
colnames(data_tmp)[18] <- 'trackingTracing.score'
colnames(data_tmp)[19] <- 'trackingTracing.rank'
colnames(data_tmp)[20] <- 'timeliness.score'
colnames(data_tmp)[21] <- 'timeliness.rank'
View(data_tmp)
data_tmp <- data_tmp[-c(1, 2), ]
View(data_tmp)
data_tmp <- read.xlsx("~/MOOC/Developing_Data_Products/data/International_LPI_from_2007_to_2014-2.xlsx", sheetName = "2014", startRow = 2, header = TRUE)
View(data_tmp)
data_tmp <- read.xlsx("~/MOOC/Developing_Data_Products/data/International_LPI_from_2007_to_2014-2.xlsx", sheetName = "2014", startRow = 3, header = FALSE)
View(data_tmp)
data_tmp <- read.xlsx("~/MOOC/Developing_Data_Products/data/International_LPI_from_2007_to_2014-2.xlsx", sheetName = "2014", startRow = 3, header = FALSE)
View(data_tmp)
data_tmp <- read.xlsx("~/MOOC/Developing_Data_Products/data/International_LPI_from_2007_to_2014-2.xlsx", sheetName = "2014", startRow = 3)
View(data_tmp)
colnames(data_tmp)[1] <- 'country'
colnames(data_tmp)[2] <- 'coountry.code'
colnames(data_tmp)[3] <- 'overall.LPI.score'
colnames(data_tmp)[4] <- 'overall.LPI.score.lowerBound'
colnames(data_tmp)[5] <- 'overall.LPI.score.upperBound'
colnames(data_tmp)[6] <- 'overall.LPI.rank'
colnames(data_tmp)[7] <- 'overall.LPI.rank.lowerBound'
colnames(data_tmp)[8] <- 'overall.LPI.rank.upperBound'
colnames(data_tmp)[9] <- 'overall.LPI.rank.percentHighestPerformer'
colnames(data_tmp)[10] <- 'customs.score'
colnames(data_tmp)[11] <- 'customs.rank'
colnames(data_tmp)[12] <- 'infrastructure.score'
colnames(data_tmp)[13] <- 'infrastructure.rank'
colnames(data_tmp)[14] <- 'international.shipment.score'
colnames(data_tmp)[15] <- 'international.shipment.rank'
colnames(data_tmp)[16] <- 'logisticsQualityCompetence.score'
colnames(data_tmp)[17] <- 'logisticsQualityCompetence.rank'
colnames(data_tmp)[18] <- 'trackingTracing.score'
colnames(data_tmp)[19] <- 'trackingTracing.rank'
colnames(data_tmp)[20] <- 'timeliness.score'
colnames(data_tmp)[21] <- 'timeliness.rank'
View(data_tmp)
equire(data.table)
require(xlsx)
# library(sqldf)
library(dplyr)
library(DT)
library(rCharts)
data_tmp <- read.xlsx("~/MOOC/Developing_Data_Products/data/International_LPI_from_2007_to_2014-2.xlsx", sheetName = "2014", startRow = 3)
View(data_tmp)
colnames(data_tmp)[1] <- 'country'
colnames(data_tmp)[2] <- 'country.code'
colnames(data_tmp)[3] <- 'overall.LPI.score'
colnames(data_tmp)[4] <- 'overall.LPI.score.lowerBound'
colnames(data_tmp)[5] <- 'overall.LPI.score.upperBound'
colnames(data_tmp)[6] <- 'overall.LPI.rank'
colnames(data_tmp)[7] <- 'overall.LPI.rank.lowerBound'
colnames(data_tmp)[8] <- 'overall.LPI.rank.upperBound'
colnames(data_tmp)[9] <- 'overall.LPI.rank.percentHighestPerformer'
colnames(data_tmp)[10] <- 'customs.score'
colnames(data_tmp)[11] <- 'customs.rank'
colnames(data_tmp)[12] <- 'infrastructure.score'
colnames(data_tmp)[13] <- 'infrastructure.rank'
colnames(data_tmp)[14] <- 'international.shipment.score'
colnames(data_tmp)[15] <- 'international.shipment.rank'
colnames(data_tmp)[16] <- 'logisticsQualityCompetence.score'
colnames(data_tmp)[17] <- 'logisticsQualityCompetence.rank'
colnames(data_tmp)[18] <- 'trackingTracing.score'
colnames(data_tmp)[19] <- 'trackingTracing.rank'
colnames(data_tmp)[20] <- 'timeliness.score'
colnames(data_tmp)[21] <- 'timeliness.rank'
